---
title: 'Exploring Quantization-Aware Training with BitNet'
date: 2024-10-10
permalink: /posts/2024/10/BitNet/
tags:
  - BitNet
  - QAT
  - quantization
---


As deep learning models grow larger and more complex, the need for optimization becomes more crucial, especially when deploying models on devices with limited memory and processing power. **Quantization-Aware Training (QAT)** is a technique aimed at reducing both the memory footprint and computation required, while maintaining performance levels. One exciting implementation is **BitNet**, which reduces model weights to just three possible values: **{-1, 0, 1}**, greatly simplifying computation. In this post, we'll dive into QAT, explain **Straight-Through Estimator (STE)**, and offer some code to get you started.


## What is Quantization?

Quantization is the process of reducing the (computational or memory) complexity of neural
networks by lowering the number of bits used to represent the parameters of a model. Most qunatization techniques focus on post-training quantization (PTQ) which trains the models in full-precision and then quantizes the weights. Quantization-aware training (QAT) includes the quantization prcoess in the training phase. The BitNet model qunatizes both the weights and the activations.

## Quantization of weights 

The following quantization functions are used for the weights \[1\]:

$$\mathrm{RoundClip}(w, a, b) = \max(a, \min(b, \text{round}(w)))$$

$$\widetilde{W} = \mathrm{RoundClip}\left( \frac{W}{\beta + \epsilon}, -1, 1 \right), \quad \text{and } \beta = \frac{1}{nm} \sum_{ij} |W_{ij}|.$$

The Roundclip function rounds a float to the nearest integer, and the values greater than $a$ and less than $b$ get clipped. $\beta$ represents the mean of the absolute values of the weight matrix $W$ and $\widetilde{W}$ the weight matrix with quantized values to \{-1,0,1\}. $\epsilon$ is added to avoid division by 0.

## Quantization of activations 

The activations (the input tensors multiplied with the weight matrices) are scaled to $[-Q_b, Q_b]$, where $Q_b = 2^{b-1}$ and $b$ is the number of bits being used (fixed to 8 in our experiments), using the following formulas \[1\]:

$$  \text{Clip}(x, a, b) = \max(a, \min(b, x)), \quad \gamma = \|x\|_\infty, \quad \text{and}$$

$$    \tilde{x} = \mathrm{Quant}(x) = \mathrm{Clip}\left(x \times \frac{Q_b}{\gamma}, -Q_b + \epsilon, Q_b - \epsilon\right).$$

In the above equations, $\tilde{x}$ represents the quantized input $x$, and $\gamma$ is the infinite norm of the input $x$ (maximum of the absolute value of each component of $x$).

## Fake Quantization

In QAT we utilze "fake" quantization. Instead of multiplying the full-precision FP32 weight matrix $W$ with the inputs $x$, the weights are first quantized to a lower-precision format (ternary values in the case of the BitNet model), then multiplied with the input (fast operation), and finally the output is dequantized. The operation can be modeled by the following expression:

$$W \cdot x \approx d(q(W)) \cdot d(q(x)) = \beta q(W) \cdot \gamma q(x) = q(W) \cdot q(x) \cdot \beta \cdot \gamma$$

where $d$ and $q$ are the dequantize and quantize operations respectively. $\beta$ annd $\gamma$ are the values from the previous equations and are used as the dequantization factors.

### Example of a forward pass 

![test](../../../../images/matmul.png "Example of a forward pass using BitNet")

## Straight-Through Estimator (STE)

Quantization, by its nature, is a non-differentiable operation, which presents a problem for backpropagation, the standard way neural networks learn. The **Straight-Through Estimator (STE)** provides an elegant workaround by allowing us to bypass the non-differentiability issue, treating quantized weights during the backward pass as though they're continuous.

$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \tilde{w}}
$$

Here, $\tilde{w}$ is the quantized weight, and the gradient is computed as if there were no quantization at all, enabling training to proceed. This trick allows us to effectively train quantized models like **BitNet**.

## Code for Training with STE

Here is the PyTorch implementation of ternary quantization using STE (this function is part of a "BitLinear" class): \[2\]

```python
def forward(self, input):

    quant_input, gamma = activation_quant(input, self.input_bits) 
    # input_bits are set to 8
    quant_weight, beta = weight_quant(self.weight)

    # implementation of the STE
    # detach() removes the quantization from the computational graph
    quant_input = input + (quant_input - input).detach()
    quant_weight = self.weight + (quant_weight - self.weight).detach()
    
    out = F.linear(quant_input, quant_weight)/ beta / gamma
    # The F.linear should be replaced by a custom kernel function to observe the improvements
    
    if self.bias is not None:
        out += self.bias.to(device).view(1, -1).expand_as(out)

    return out

```


## References

1. **BitNet** - Ma et al. 2024. *The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits*. Available at: [link_to_paper](https://arxiv.org/pdf/2402.17764).
2. **BitNet FAQ** - Ma et al. 2024. [github_repo](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf).